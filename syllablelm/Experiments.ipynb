{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1c2efc",
   "metadata": {},
   "source": [
    "# Experiments.ipynb\n",
    "\n",
    "This contains poorly maintained code for\n",
    "\n",
    " - Interleaved Model Resynthesis\n",
    " - LossPred\n",
    " - Boundary Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d26382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.ticker as ticker\n",
    "import os\n",
    "import csv\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "sys.path.append('/home/abaade/MAE-AST-Playground')\n",
    "import fairseq\n",
    "from fairseq import models, checkpoint_utils\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/data/scratch/abaade/cluster/hubert_large_ll60k.pt\"\n",
    "hubert_large_models, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([ckpt_path])\n",
    "hubert_large_model = hubert_large_models[0]\n",
    "hubert_large_model = hubert_large_model.eval().cuda()\n",
    "\n",
    "ckpt_path = \"/data/scratchre/abaade/hubert/hubert_base_ls960.pt\"\n",
    "hubert_base_models, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([ckpt_path])\n",
    "hubert_base_model = hubert_base_models[0]\n",
    "hubert_base_model = hubert_base_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('/data/scratch/abaade/cluster/SegmentalSSE/exp/alan_a40_ls_runAve0_crf1_crfWei1_bceWei0_freezeSeg0_pretrain_faTrain0_posWeight3_useFa0_segQuan0.95_ancimage_cls1_comLayer1_9_clsPolicy0.00001_featPolicy-10202_clsMatch4_featMatch0_aweFreeze0/bundle.pth')\n",
    "ckpt_path = \"/data/scratch/abaade/hubert/hubert_base_ls960.pt\"\n",
    "mean_base_models, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([ckpt_path])\n",
    "mean_base_model_new2 = mean_base_models[0]\n",
    "mean_base_model_new2 = mean_base_model_new2.eval().cuda()\n",
    "mean_base_model_new2.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8de7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.data2vec.models import data2vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d2v2_model_dev, _, _ = checkpoint_utils.load_model_ensemble_and_task(\n",
    "    [\"/data/scratch/abaade/data2vec2/base_libri.pt\"],\n",
    ")\n",
    "d2v2_model_dev = d2v2_model_dev[0].cuda().eval().half()\n",
    "state_dict = torch.load('/data/scratch/abaade/cluster/SegmentalSSE/exp/alan_a40_ls_runAve0_crf1_crfWei1_bceWei0_freezeSeg0_pretrain_faTrain0_posWeight3_useFa0_segQuan0.95_ancimage_cls1_comLayer1_9_clsPolicy0.00001_featPolicy-2120_clsMatch4_featMatch0_aweFreeze0/bundle.pth')\n",
    "d2v2_model_dev.load_state_dict({k[len('model.'):]: v for k,v in state_dict['model_seg'].items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc89bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.classifier import Classifier, get_features\n",
    "import omegaconf\n",
    "cfg = omegaconf.OmegaConf.load('./config/conf.yaml')\n",
    "from utils.misc import load_from_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48544763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mean(x, lst):\n",
    "    # if len(x) == 0:\n",
    "    #     return\n",
    "    # elif len(x) == 1:\n",
    "    #     lst.append(0.0)\n",
    "    # else:\n",
    "    #     for _ in range(len(x)):\n",
    "    #         lst.append(torch.stack(x).var(dim=0).mean().detach().cpu().item())\n",
    "    \n",
    "    if len(x) == 5:\n",
    "        for _ in range(len(x)):\n",
    "            lst.append(torch.stack(x).var(dim=0).mean().detach().cpu().item())\n",
    "\n",
    "def phone_class_file(audio_file, pred_override=None, p=0.5):\n",
    "    with torch.no_grad():\n",
    "        wavs, sr = torchaudio.load(audio_file)\n",
    "        assert sr == 16000\n",
    "        wavs = wavs.cuda()\n",
    "\n",
    "        results = model.forward(wavs, padding_mask=None, features_only=True, mask=False)\n",
    "        \n",
    "        if pred_override is None:\n",
    "            results[\"layer_results\"] = results[\"hidden_states\"]\n",
    "            features = get_features(results, cfg.mode)\n",
    "            preds = classifier(features).squeeze()\n",
    "            preds = F.sigmoid(preds)\n",
    "            preds = preds > p\n",
    "        else:\n",
    "            preds = torch.tensor(pred_override)\n",
    "        \n",
    "        cur_a = []\n",
    "        cur_b = []\n",
    "        outs_a = []\n",
    "        outs_b = []\n",
    "        layer_result = results['hidden_states'][8][0].transpose(0,1)[0]\n",
    "        \n",
    "        lr = layer_result\n",
    "        z = (lr - lr.mean(dim=-1, keepdim=True))/lr.std(dim=-1, keepdim=True)\n",
    "        layer_result = z\n",
    "        \n",
    "        print(preds.shape, layer_result.shape)\n",
    "        for idx in range(layer_result.size(0)):\n",
    "            cur_x = layer_result[idx]\n",
    "            \n",
    "            cur_a.append(cur_x)\n",
    "            if preds[idx]:\n",
    "                add_mean(cur_a, outs_a)\n",
    "                add_mean(cur_b, outs_b)\n",
    "                cur_a = []\n",
    "                cur_b = []\n",
    "            cur_b.append(cur_x)\n",
    "                    \n",
    "        add_mean(cur_a, outs_a)\n",
    "        add_mean(cur_b, outs_b)\n",
    "\n",
    "    \n",
    "    return preds, outs_a, outs_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ali(raw_ali, fps=50):\n",
    "    \"\"\"\n",
    "    raw_ali is a string like 'start1__word1__end1 start2__word2__end2 ...'\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    meta_toks = raw_ali.split()\n",
    "    for meta_tok in meta_toks:\n",
    "        toks = meta_tok.split('__')\n",
    "        if len(toks) == 3:\n",
    "            data.append((float(toks[0])*fps, float(toks[2])*fps, toks[1]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a93ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_featmap(feat, word_ali, start_in_sec=0, fps=50, image_already=None,\n",
    "                 boundary=None, cmap=None, figsize=(15,15), save_to=None,\n",
    "                 extend_words=False, do_yaxis=True, kill_last=False,\n",
    "                ):    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    if image_already is not None:\n",
    "        ax.imshow(image_already, cmap=cmap) # TODO why inversing it?\n",
    "    else:\n",
    "        ax.imshow(feat@feat.transpose(1,0), cmap=cmap) # TODO why inversing it?\n",
    "    if word_ali != None:\n",
    "        ticks = []\n",
    "        minor_ticks = []\n",
    "        labels = []\n",
    "        for i, item in enumerate(word_ali):\n",
    "            if item[0] - fps*start_in_sec >= -1:\n",
    "                ticks.append(item[0] - fps*start_in_sec)\n",
    "                labels.append(item[2].lower())\n",
    "                minor_ticks.append((item[1] + item[0])//2-fps*start_in_sec)\n",
    "        ticks.append(word_ali[-1][1])\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator(ticks))\n",
    "        ax.xaxis.set_minor_locator(ticker.FixedLocator(minor_ticks))\n",
    "\n",
    "        ax.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "        ax.xaxis.set_minor_formatter(ticker.FixedFormatter(labels))\n",
    "\n",
    "        for i, tick in enumerate(ax.xaxis.get_minor_ticks()):\n",
    "            if i % 2 == 0:\n",
    "                tick.set_pad(17)  # Increase or decrease the value to adjust the height\n",
    "            else:\n",
    "                tick.set_pad(3)  # Increase or decrease the value to adjust the height\n",
    "            tick.tick1line.set_markersize(0)\n",
    "            tick.tick2line.set_markersize(0)\n",
    "            tick.label1.set_horizontalalignment('center')\n",
    "\n",
    "        if do_yaxis:\n",
    "            ax.yaxis.set_major_locator(ticker.FixedLocator(ticks))\n",
    "            ax.yaxis.set_minor_locator(ticker.FixedLocator(minor_ticks))\n",
    "\n",
    "            ax.yaxis.set_major_formatter(ticker.NullFormatter())\n",
    "            ax.yaxis.set_minor_formatter(ticker.FixedFormatter(labels))\n",
    "        else:\n",
    "            ax.yaxis.set_major_locator(ticker.NullLocator())\n",
    "            ax.yaxis.set_minor_locator(ticker.NullLocator())\n",
    "\n",
    "            ax.yaxis.set_major_formatter(ticker.NullFormatter())\n",
    "            ax.yaxis.set_minor_formatter(ticker.NullFormatter())\n",
    "\n",
    "\n",
    "        for tick in ax.yaxis.get_minor_ticks():\n",
    "            tick.tick1line.set_markersize(0)\n",
    "            tick.tick2line.set_markersize(0)\n",
    "            tick.label1.set_horizontalalignment('center')\n",
    "        ax.tick_params(axis='y', which='minor', rotation=90)\n",
    "        if extend_words:\n",
    "            (ymin, ymax) = ax.get_ylim()\n",
    "            for b in (ticks[:-1] if kill_last else ticks):\n",
    "                ax.vlines(x=b, ymin = ymin, ymax=((ymin+ymax)*0.93 if extend_words else ymax), colors='red', ls='-', lw=3)\n",
    "        if boundary is not None:\n",
    "            (ymin, ymax) = ax.get_ylim()\n",
    "            for b in boundary:\n",
    "                ax.vlines(x=b, ymin = ((ymin+ymax)*0.93 if extend_words else ymin), ymax=ymax, colors='#AA0000', ls='-', lw=3)\n",
    "                \n",
    "                 \n",
    "        for label in ax.get_xticklabels(minor=True):\n",
    "            label.set_fontsize(20)\n",
    "        ax.tick_params(axis='x', which='major', length=20, width=3, colors='#0000AA')\n",
    "\n",
    "                \n",
    "        if save_to is not None:\n",
    "            plt.tight_layout()\n",
    "#             plt.gca().set_axis_off()\n",
    "            plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "                        hspace = 0, wspace = 0)\n",
    "            plt.margins(0,0)\n",
    "            plt.savefig(save_to, bbox_inches='tight')\n",
    "            plt.clf()\n",
    "        else:\n",
    "            plt.show()\n",
    "            \n",
    "from PIL import Image\n",
    "def concatenate_vertically(image1_path, image2_path, output_path):\n",
    "    image1 = Image.open(image1_path)\n",
    "    image2 = Image.open(image2_path)\n",
    "\n",
    "    if image1.width != image2.width:\n",
    "        raise ValueError(\"Images must have the same width for vertical concatenation.\")\n",
    "\n",
    "    new_image = Image.new(\"RGB\", (image1.width, image1.height + image2.height))\n",
    "    new_image.paste(image1, (0, 0))\n",
    "    new_image.paste(image2, (0, image1.height))\n",
    "    new_image.save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26287fe3",
   "metadata": {},
   "source": [
    "## Annoying Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbb249c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/abaade/MAE-AST-Playground/examples/hubert/simple_kmeans/')\n",
    "\n",
    "from examples.hubert.simple_kmeans.alan_good_kmeans import HubertFeatureReader, ApplyKmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ecb8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "# import seaborn as sns\n",
    "import torchaudio\n",
    "import time\n",
    "import soundfile as sf\n",
    "import sys\n",
    "sys.path.append('/home/abaade/MAE-AST-Playground/')\n",
    "sys.path.append('/home/abaade/MAE-AST-Playground/examples')\n",
    "import examples\n",
    "sys.path.append('/home/abaade/MAE-AST-Playground/examples/data2vec')\n",
    "sys.path.append('/home/abaade/MAE-AST-Playground/examples/data2vec/models')\n",
    "\n",
    "from fairseq import checkpoint_utils\n",
    "\n",
    "import data2vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ffa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "hubert_akm = ApplyKmeans('/data/scratch/abaade/cluster/hubert_base_ls960_L9_km500.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a0900",
   "metadata": {},
   "source": [
    "## Twist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78939d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "sys.path.append('/home/abaade/textlesslib')\n",
    "sys.path.append('/home/abaade/textlesslib/examples/twist')\n",
    "sys.path.append('/home/abaade/textlesslib/textless/data')\n",
    "sys.path.append('/home/abaade/textlesslib/textless')\n",
    "\n",
    "from textless.data.speech_encoder import SpeechEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08992911",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model, quantizer_model, vocab = \"mhubert-base-25hz\", \"kmeans\", 500\n",
    "encoder_25hz = SpeechEncoder.by_name(\n",
    "    dense_model_name = dense_model,\n",
    "    quantizer_model_name = quantizer_model,\n",
    "    vocab_size = vocab,\n",
    "    deduplicate=False,\n",
    "    need_f0=False,\n",
    "    add_bos_eos=False,\n",
    ").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d96bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "twist_lm_model = AutoModelForCausalLM.from_pretrained('/home/abaade/textlesslib/textlesslibexamples/twist/ckpts/TWIST-350M/')\n",
    "# twist_lm_model = AutoModelForCausalLM.from_pretrained('/data/scratch/abaade/textlesslib/TWIST-1.3B/')\n",
    "twist_lm_model = twist_lm_model.cuda()\n",
    "twist_lm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 13:38:15 | INFO | fairseq.tasks.cluster_alan_pretraining | current directory is /home/abaade/phone-seg\n",
      "2025-02-23 13:38:15 | INFO | fairseq.tasks.cluster_alan_pretraining | ClusterAlanPretrainingConfig Config {'_name': 'cluster_alan_pretraining', 'data': '/data/scratch/abaade/cluster/tsv/libri_light_bulk_mhubert25hz/', 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 208005, 'min_sample_size': 0, 'min_keep_phone_size': 50, 'random_crop': True, 'feature_type': 'wav', 'encodec_rate': 75, 'full_prompt_type': 'norm', 'pred_prompt_type': 'encodec', 'reducer_path': None, 'tsv_replace_source': None, 'tsv_replace_target': None, 'bpe_path': '/data/scratch/abaade/cluster/acoustic_bpe/tokenizer_4096.json', 'vocab_size': 4096}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ours_ckpt_mindist = '/data/scratch/abaade/cluster/models/syl_d2v2_LibriSpeech_bulk_2212_24576_agg_8192_equal_compute_april_22/checkpoints/checkpoint_last.pt'\n",
    "\n",
    "ours_lm_model_mindist, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([ours_ckpt_mindist])\n",
    "ours_lm_model_mindist = ours_lm_model_mindist[0]\n",
    "ours_lm_model_mindist = ours_lm_model_mindist.cuda()\n",
    "ours_lm_model_mindist = ours_lm_model_mindist.eval()\n",
    "ours_lm_model_mindist = ours_lm_model_mindist.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b380e3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 13:27:28 | INFO | fairseq.tasks.cluster_alan_interleaved_pretraining | current directory is /home/abaade/phone-seg\n",
      "2025-02-23 13:27:28 | INFO | fairseq.tasks.cluster_alan_interleaved_pretraining | ClusterAlanPretrainingConfig Config {'_name': 'cluster_alan_interleaved_pretraining', 'data': '/data/scratch/abaade/cluster/tsv/libri_light_full_interleave/', 'sample_rate': 16000, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 208005, 'min_sample_size': 0, 'min_keep_phone_size': 50, 'random_crop': True, 'feature_type': 'wav', 'encodec_rate': 75, 'full_prompt_type': 'norm', 'pred_prompt_type': 'encodec', 'vocab_size_short': 1024, 'vocab_size_long': 8192, 'interleave_strat': 5, 'long_dropout': 0.0, 'reducer_path': '/saltpool0/scratch/abaade/cluster/LibriSpeech_bulk_2116_24576_agg_8192.npy', 'tsv_replace_source': 'LibriSpeech_bulk_2120_32768_w23_0019', 'tsv_replace_target': 'LibriSpeech_bulk_2116_24576', 'interleave_long_grouping': 1}\n"
     ]
    }
   ],
   "source": [
    "ours_ckpt_resynthesis = '/data/scratch/abaade/cluster/models/syl_d2v2_interleaved5_LibriSpeech_bulk_2116_24576_8192/checkpoints/checkpoint_last.pt'\n",
    "\n",
    "ours_lm_model_resynthesis, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([ours_ckpt_resynthesis])\n",
    "ours_lm_model_resynthesis = ours_lm_model_resynthesis[0]\n",
    "ours_lm_model_resynthesis = ours_lm_model_resynthesis.cuda()\n",
    "ours_lm_model_resynthesis = ours_lm_model_resynthesis.eval()\n",
    "ours_lm_model_resynthesis = ours_lm_model_resynthesis.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d42ebe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(fairseq.data.audio.cluster_alan_dataset_interleaved)\n",
    "from fairseq.data.audio.cluster_alan_dataset_interleaved import ClusterAlanDatasetInterleaved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf494e",
   "metadata": {},
   "source": [
    "## RESYNTHESIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e84aed79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      "CodeHiFiGAN model loaded!\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "import textless\n",
    "dense_model, quantizer_model, vocab = \"mhubert-base-25hz\", \"kmeans\", 500\n",
    "vocoder = textless.CodeHiFiGANVocoder.by_name(\n",
    "    dense_model_name = dense_model,\n",
    "    quantizer_model_name = quantizer_model,\n",
    "    vocab_size = vocab\n",
    ").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bda79c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def resynth(tokens_mhubert, out_path='/home/abaade/temp.wav', max_len=None):\n",
    "    out_audio = vocoder(tokens_mhubert, dur_prediction = True)\n",
    "    out_audio = out_audio[:max_len] if max_len is not None else out_audio\n",
    "    torchaudio.save(out_path, out_audio.cpu().unsqueeze(0)[:,:], 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bda7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data.audio.cluster_alan_dataset_interleaved import ClusterAlanDatasetInterleaved\n",
    "# interleaved_dataset = ClusterAlanDatasetInterleaved(None, interleave_strat=3, vocab_size_long=4096)\n",
    "interleaved_dataset = ClusterAlanDatasetInterleaved(\n",
    "    None, interleave_strat=5, \n",
    "#     vocab_size_long=2048, reducer_path='/saltpool0/scratch/abaade/cluster/LibriSpeech_bulk_2212_24576_agg_2048.npy',\n",
    "    vocab_size_long=8192, reducer_path='/saltpool0/scratch/abaade/cluster/LibriSpeech_bulk_2116_24576_agg_8192.npy',\n",
    "#     vocab_size_long=16384, reducer_path='/saltpool0/scratch/abaade/cluster/LibriSpeech_bulk_2120_24576_16384_w23.npy',\n",
    "#     vocab_size_long=4096, reducer_path='/saltpool0/scratch/abaade/cluster/LibriSpeech_bulk_2120_24576_4096_w23.npy',\n",
    "#     vocab_size_long=4096, reducer_path='/saltpool0/scratch/abaade/cluster/LibriSpeech_bulk_1120_16384_4096.npy',\n",
    "    tsv_replace_source='QWRQWRQWRQWR',\n",
    "    tsv_replace_target='QWRQWRQWRQWR',\n",
    ")\n",
    "task = \"tSC\"\n",
    "mode = 'dev'\n",
    "zs_dir = f'/data/scratch/abaade/cluster/{task}/'\n",
    "zs_lengths_file = f'/data/scratch/abaade/cluster/{task}/{mode}.tsv'\n",
    "\n",
    "\n",
    "audio_file = \"/data/scratch/abaade/cluster/tSC/\" + [\n",
    "\"0_138d5bfb-05cc-41e3-bf2c-fa85ebad14e2_correct.wav\",\n",
    "\"1000_7ab5d7b2-f587-433f-b9b0-79a841cf0753_correct.wav\",\n",
    "\"1001_7ab5d7b2-f587-433f-b9b0-79a841cf0753_incorrect.wav\",\n",
    "\"1002_5a4e4ff0-d972-4991-8f91-bcf4dae1288e_correct.wav\",\n",
    "\"1003_5a4e4ff0-d972-4991-8f91-bcf4dae1288e_incorrect.wav\",\n",
    "\"1004_2d1c5e85-93b9-41a6-8c0f-f8e6035fe78e_correct.wav\",\n",
    "\"1005_2d1c5e85-93b9-41a6-8c0f-f8e6035fe78e_incorrect.wav\",\n",
    "\"100_57d5152a-77db-4156-8330-166edb36b0a2_correct.wav\",\n",
    "\"1006_069fe406-47af-4c57-ae9c-2ffea2a0f7ee_correct.wav\",\n",
    "\"1007_069fe406-47af-4c57-ae9c-2ffea2a0f7ee_incorrect.wav\",\n",
    "][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3f804cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucleus_sample(logits, p=0.9, m=25):\n",
    "    raw_probs = logits.softmax(dim=-1)\n",
    "    sorted_indices = raw_probs.argsort(dim=-1, descending=True)\n",
    "    too_small_indices = sorted_indices[1:][(raw_probs[sorted_indices].cumsum(-1) > p)[:-1]]\n",
    "    nucleus_logits = logits.clone()\n",
    "    nucleus_logits[too_small_indices] = -1000\n",
    "    nucleus_logits[sorted_indices[m:]] = -1000\n",
    "    nucleus_probs = nucleus_logits.softmax(dim=-1)\n",
    "    return np.random.choice(np.arange(len(nucleus_probs)), p=nucleus_probs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97ec4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_sample(logits, t=1.0):\n",
    "    scaled_logits = logits/t\n",
    "    probs = scaled_logits.softmax(dim=-1)\n",
    "    return np.random.choice(np.arange(len(probs)), p=probs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### RESYNTH WITH LEARNED STOP TOKENS\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_cache(model, full_tokens, pasts):\n",
    "#     print(pasts.shape if pasts is not None else None)\n",
    "    offset = 0 if pasts is None else pasts[0].size(-2)\n",
    "    input_tokens = full_tokens[:, offset:]\n",
    "    features = model.embed_weight[input_tokens]\n",
    "    features = model.pos_embed(features, offset=offset)\n",
    "\n",
    "    x, hidden_states, presents = model.encoder.forward_cache(features, padding_mask=None, pasts=pasts)\n",
    "    logits = model.embed_weight @ x.transpose(-1, -2)\n",
    "    pasts = presents if pasts is None else torch.cat([pasts, presents], dim=-2)\n",
    "    \n",
    "    return {'logits': logits}, pasts\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def resynth_slow(sd_path, out_path):\n",
    "#     raw_root = audio_file[:audio_file.rfind('/')]\n",
    "#     raw_filename = audio_file[audio_file.rfind('/'):-len('.wav')] + '.npy'\n",
    "#     mh_path = raw_root + '_clusters/mhubert_25hz' + raw_filename\n",
    "#     sd_path = raw_root + '_clusters/LibriSpeech_bulk_2212_24576' + raw_filename\n",
    "#     sd_path = raw_root + '_clusters/LibriSpeech_bulk_2120_24576_w23_0019' + raw_filename\n",
    "#     sd_path = raw_root + '_clusters/sdhubert' + raw_filename\n",
    "#     sd_path = '/data/scratch/abaade/cluster/librispeech_clusters/test/LibriSpeech_bulk_2212_24576/1995/1836/1995-1836-0006.npy'\n",
    "\n",
    "    input_tokens_long = interleaved_dataset.reducer[np.load(sd_path)[0]] + interleaved_dataset.vocab_size_short + 1\n",
    "#     input_tokens_long = np.array(output_long_tokens[1:]) + interleaved_dataset.vocab_size_short + 1\n",
    "    \n",
    "    magic_max_tokens = int(15*22.0)\n",
    "    magic_true_num_short = 500\n",
    "    magic_stop_idx = 1024\n",
    "    magic_long_end_idx = interleaved_dataset.pad_idx-1\n",
    "    \n",
    "    long_token_idx = 0\n",
    "    interleaved_tokens_list = []\n",
    "    state = 'add_long'\n",
    "#     'finish_up'\n",
    "#     'add_short'\n",
    "    finish_up_countdown = 1 if '2212' in sd_path else 2\n",
    "    running_stop_prob = 1.\n",
    "\n",
    "    pasts = None\n",
    "    output_short_tokens = []\n",
    "    \n",
    "    for _ in range(magic_max_tokens+10):\n",
    "        if state == 'add_long':\n",
    "            interleaved_tokens_list.extend([input_tokens_long[long_token_idx], magic_long_end_idx])\n",
    "            interleaved_tokens = torch.tensor(interleaved_tokens_list, device='cuda', dtype=int).view(1,-1)\n",
    "            \n",
    "            out_dict = ours_lm_model_resynthesis(interleaved_tokens, features_only=True)\n",
    "#             out_dict, pasts = forward_cache(ours_lm_model_resynthesis, interleaved_tokens, pasts)\n",
    "            \n",
    "            logits = out_dict['logits']\n",
    "            logits = logits.float()\n",
    "            logits[:,magic_true_num_short:,:] = -100\n",
    "            cur_short_logits = logits[0,:,-1].softmax(dim=0)\n",
    "            next_prob_topk = cur_short_logits.topk(1)  # no stop token\n",
    "            next_short_token = np.random.choice(\n",
    "                next_prob_topk.indices.cpu().numpy(), \n",
    "                p=next_prob_topk.values.cpu().numpy()/next_prob_topk.values.cpu().numpy().sum()\n",
    "            )\n",
    "            \n",
    "            output_short_tokens.append(next_short_token)\n",
    "            interleaved_tokens_list.append(next_short_token)\n",
    "            \n",
    "            long_token_idx += 1\n",
    "#             if long_token_idx == len(input_tokens_long):\n",
    "#                 state = 'finish_up'\n",
    "#             else:\n",
    "            state = 'add_short'\n",
    "        elif state == 'add_short':\n",
    "            interleaved_tokens = torch.tensor(interleaved_tokens_list, device='cuda', dtype=int).view(1,-1)\n",
    "            \n",
    "            out_dict = ours_lm_model_resynthesis(interleaved_tokens, features_only=True)\n",
    "#             out_dict, pasts = forward_cache(ours_lm_model_resynthesis, interleaved_tokens, pasts)\n",
    "            logits = out_dict['logits']\n",
    "            logits = logits.float()\n",
    "            \n",
    "            logits[:,magic_true_num_short:magic_stop_idx,:] = -100\n",
    "            logits[:,magic_stop_idx+1:,:] = -100\n",
    "            \n",
    "            cur_short_logits = logits[0,:,-1].softmax(dim=0)\n",
    "            running_stop_prob *= (1-cur_short_logits[magic_stop_idx])\n",
    "            if running_stop_prob < (0.9 if long_token_idx == len(input_tokens_long) else 0.5):\n",
    "                is_stop = True\n",
    "            elif (len(output_short_tokens) / magic_max_tokens > long_token_idx / len(input_tokens_long)) and len(output_short_tokens) > 3:\n",
    "                is_stop = True\n",
    "                print('FORCED_END')\n",
    "            else:\n",
    "                is_stop = False\n",
    "\n",
    "            logits[:,magic_stop_idx,:] = -100\n",
    "            cur_short_logits = logits[0,:,-1].softmax(dim=0)\n",
    "            next_prob_topk = cur_short_logits.topk(1)\n",
    "            next_short_token = np.random.choice(\n",
    "                next_prob_topk.indices.cpu().numpy(), \n",
    "                p=next_prob_topk.values.cpu().numpy()/next_prob_topk.values.cpu().numpy().sum()\n",
    "            )\n",
    "            \n",
    "            if is_stop:\n",
    "                running_stop_prob = 1.\n",
    "                if long_token_idx == len(input_tokens_long):\n",
    "                    output_short_tokens.append(next_short_token)\n",
    "                    interleaved_tokens_list.append(next_short_token)\n",
    "                    state = 'finish_up'\n",
    "                else:\n",
    "                    state = 'add_long'\n",
    "            else:\n",
    "                output_short_tokens.append(next_short_token)\n",
    "                interleaved_tokens_list.append(next_short_token)\n",
    "        elif state == 'finish_up':\n",
    "            interleaved_tokens = torch.tensor(interleaved_tokens_list, device='cuda', dtype=int).view(1,-1)\n",
    "            \n",
    "            out_dict = ours_lm_model_resynthesis(interleaved_tokens, features_only=True)\n",
    "#             out_dict, pasts = forward_cache(ours_lm_model_resynthesis, interleaved_tokens, pasts)\n",
    "            logits = out_dict['logits']\n",
    "            logits = logits.float()\n",
    "            \n",
    "            logits[:,magic_true_num_short:,:] = -100  # no stop token\n",
    "            cur_short_logits = logits[0,:,-1].softmax(dim=0)\n",
    "            next_prob_topk = cur_short_logits.topk(1)\n",
    "            next_short_token = np.random.choice(\n",
    "                next_prob_topk.indices.cpu().numpy(), \n",
    "                p=next_prob_topk.values.cpu().numpy()/next_prob_topk.values.cpu().numpy().sum()\n",
    "            )\n",
    "            \n",
    "            output_short_tokens.append(next_short_token)\n",
    "            interleaved_tokens_list.append(next_short_token)\n",
    "            \n",
    "            finish_up_countdown -= 1\n",
    "            if finish_up_countdown == 0:\n",
    "                break\n",
    "        \n",
    "    resynth(torch.tensor(output_short_tokens).cuda(), out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tsv = '/data/scratch/abaade/valle/evaluation/wer/data/test_4_to_10.tsv'\n",
    "out_dir = '/data/scratch/abaade/valle/evaluation/output_audio/cluster_test_1120_4096a/'\n",
    "ltr_tsv = '/data/scratch/abaade/valle/evaluation/wer/data/test_4_to_10.ltr'\n",
    "in_npy = '/data/scratch/abaade/cluster/librispeech_clusters/test/LibriSpeech_bulk_2116_24576/'\n",
    "# in_npy = '/data/scratch/abaade/cluster/librispeech_clusters/test/LibriSpeech_bulk_2120_24576_w23_0019/'\n",
    "# in_npy = '/data/scratch/abaade/cluster/librispeech_clusters/test/sdhubert/'\n",
    "# in_npy = '/data/scratch/abaade/cluster/librispeech_clusters/test/LibriSpeech_bulk_1120_16384_sdh_parity/'\n",
    "\n",
    "with open(in_tsv, 'r') as f:\n",
    "    next(f)\n",
    "    for line in tqdm(f):\n",
    "        in_path = line.split()[0]\n",
    "        in_name = in_path[len('/data/scratch/pyp/datasets/LibriSpeech/data/test-clean/'):-len('.flac')]\n",
    "        in_path_npy = in_npy + in_name + '.npy'\n",
    "        out_path = out_dir + in_name + '.flac'\n",
    "        print(in_path_npy)\n",
    "        print(out_path)\n",
    "        os.makedirs(out_path[:out_path.rfind('/')], exist_ok=True)\n",
    "        resynth_slow(in_path_npy, out_path)\n",
    "#         resynth_sdhubert_slow(in_path_npy, out_path)\n",
    "#         assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f1d4d",
   "metadata": {},
   "source": [
    "## LossPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e49094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def get_slow_coco_outs(\n",
    "    data_dict, # wavs: boundary dict\n",
    "    data_json,\n",
    "    tolerance = 0.05,\n",
    "    level=\"syllable\",\n",
    "    block_shift=False,\n",
    "):\n",
    "    def get_word_ali(raw_ali):\n",
    "        \"\"\"\n",
    "        raw_ali is a string like 'start1__word1__end1 start2__word2__end2 ...'\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        meta_toks = raw_ali.split()\n",
    "        for meta_tok in meta_toks:\n",
    "            toks = meta_tok.split('__')\n",
    "            if len(toks) == 3:\n",
    "                data.append([float(toks[0]), float(toks[2])])\n",
    "        return data\n",
    "\n",
    "    def find_boundary_matches(gt, pred, tolerance):\n",
    "        \"\"\"\n",
    "        gt: list of ground truth boundaries\n",
    "        pred: list of predicted boundaries\n",
    "        all in seconds\n",
    "        \"\"\"\n",
    "        gt_pointer = 0\n",
    "        pred_pointer = 0\n",
    "        gt_len = len(gt)\n",
    "        pred_len = len(pred)\n",
    "        match_pred = 0\n",
    "        match_gt = 0\n",
    "        while gt_pointer < gt_len and pred_pointer < pred_len:\n",
    "            if np.abs(gt[gt_pointer] - pred[pred_pointer]) <= tolerance:\n",
    "                match_gt += 1\n",
    "                match_pred += 1\n",
    "                gt_pointer += 1\n",
    "                pred_pointer += 1\n",
    "            elif gt[gt_pointer] > pred[pred_pointer]:\n",
    "                pred_pointer += 1\n",
    "            else:\n",
    "                gt_pointer += 1\n",
    "        return match_gt, match_pred, gt_len, pred_len\n",
    "\n",
    "\n",
    "\n",
    "    # first run to select systematic shift, using 1/10 of the data\n",
    "    # following O. Räsänen, G. Doyle, and M. C. Frank, “Pre-linguistic segmentation of speech into syllable-like units,” Cognition, 2018\n",
    "    tolerance = tolerance # tolerance setting follows the paper\n",
    "    shifts = np.arange(-0.06, 0.031, 0.005)\n",
    "    if block_shift:\n",
    "        shifts = np.arange(-0.0001, 0.0002, 0.0001)\n",
    "        print(shifts)\n",
    "    shift_res = {}\n",
    "    # save the segmentation result when calculating metrics, just in case we want to reuse\n",
    "    for shift in shifts:\n",
    "        match_gt_count = 0\n",
    "        match_pred_count = 0\n",
    "        gt_b_len = 0\n",
    "        pred_b_len = 0\n",
    "        for j, item in enumerate(data_json[:50]):  ## TODO ::10\n",
    "            wav_key = item['caption']['wav'] if 'caption' in item else item['wav']\n",
    "            raw_ali = get_word_ali(item[f'{level}_alignment'])\n",
    "            if len(raw_ali) == 0:\n",
    "                continue\n",
    "            gt_b = np.unique(raw_ali).tolist()\n",
    "            if wav_key not in data_dict:\n",
    "                print(f\"missing {wav_key} in data_dict!\")\n",
    "            pred_b = np.unique(data_dict[wav_key]['word_boundaries']).tolist()\n",
    "            pred_b = [bb + shift for bb in pred_b]\n",
    "            a, b, c, d = find_boundary_matches(gt_b, pred_b, tolerance)\n",
    "\n",
    "            match_gt_count += a\n",
    "            match_pred_count += b\n",
    "            gt_b_len += c\n",
    "            pred_b_len += d\n",
    "            # if j > 10:\n",
    "            #     break\n",
    "        b_prec = match_pred_count / pred_b_len + 1e-5\n",
    "        b_recall = match_gt_count / gt_b_len + 1e-5\n",
    "        b_f1 = 2*b_prec*b_recall / (b_prec+b_recall)\n",
    "        b_os = b_recall / b_prec - 1.\n",
    "        b_r1 = np.sqrt((1-b_recall)**2 + b_os**2)\n",
    "        b_r2 = (-b_os + b_recall - 1) / np.sqrt(2)\n",
    "        b_r_val = 1. - (np.abs(b_r1) + np.abs(b_r2))/2.\n",
    "        shift_res[shift] = (b_prec, b_recall, b_os, b_f1, b_r_val)\n",
    "\n",
    "    # select based on max R-val\n",
    "    all_f1 = [val[4] for val in shift_res.values()]\n",
    "    ind = np.argmax(all_f1)\n",
    "    best_shift = list(shift_res.keys())[ind]\n",
    "\n",
    "    match_gt_count = 0\n",
    "    match_pred_count = 0\n",
    "    gt_b_len = 0\n",
    "    pred_b_len = 0\n",
    "    for j, item in enumerate(data_json):\n",
    "        wav_key = item['caption']['wav'] if 'caption' in item else item['wav']\n",
    "        raw_ali = get_word_ali(item[f'{level}_alignment'])\n",
    "        if len(raw_ali) == 0:\n",
    "            continue\n",
    "        gt_b = np.unique(raw_ali).tolist()\n",
    "        if wav_key not in data_dict:\n",
    "            print(f\"missing {wav_key} in data_dict!\")\n",
    "        pred_b = np.unique(data_dict[wav_key]['word_boundaries']).tolist()\n",
    "        pred_b = [bb + best_shift for bb in pred_b]\n",
    "        a, b, c, d = find_boundary_matches(gt_b, pred_b, tolerance)\n",
    "\n",
    "        match_gt_count += a\n",
    "        match_pred_count += b\n",
    "        gt_b_len += c\n",
    "        pred_b_len += d\n",
    "    b_prec = match_pred_count / pred_b_len + 1e-5\n",
    "    b_recall = match_gt_count / gt_b_len + 1e-5\n",
    "    b_f1 = 2*b_prec*b_recall / (b_prec+b_recall)\n",
    "    b_os = b_recall / b_prec - 1.\n",
    "    b_r1 = np.sqrt((1-b_recall)**2 + b_os**2)\n",
    "    b_r2 = (-b_os + b_recall - 1) / np.sqrt(2)\n",
    "    b_r_val = 1. - (np.abs(b_r1) + np.abs(b_r2))/2.\n",
    "\n",
    "    print(\"best_shift:\", best_shift)\n",
    "    print(\"boundary precision:\", b_prec)\n",
    "    print(\"boundary recall: \", b_recall)\n",
    "    print(\"boundary F1:\", b_f1)\n",
    "    print(\"boundary over-segmentation: \", b_os)\n",
    "    print(\"boundary R value: \", b_r_val)\n",
    "    \n",
    "    return b_prec, b_recall, b_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ec759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rVADfast import rVADfast\n",
    "vad = rVADfast()\n",
    "\n",
    "def get_vad_segment_boundaries(audio_np, sr=16000):\n",
    "    vad_labels, vad_timestamps = vad(audio_np, sr)\n",
    "    start, end = vad_labels.nonzero()[0][[0,-1]]\n",
    "    end = min(end+1, vad_labels.shape[-1])\n",
    "    start_t, end_t = vad_timestamps[[start, end]]\n",
    "    return start_t, end_t\n",
    "\n",
    "def get_vad_full_boundaries(audio_np, sr=16000):\n",
    "    vad_labels, vad_timestamps = vad(audio_np, sr)\n",
    "    start, end = vad_labels.nonzero()[0][[0,-1]]\n",
    "    end = min(end+1, vad_labels.shape[-1])\n",
    "    start_t, end_t = vad_timestamps[[start, end]]\n",
    "    return start_t, end_t, (1-vad_labels).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c633def",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b6e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_cut(ssm, K, bias=0., min_hop=1, max_hop=25):\n",
    "    N = ssm.shape[0]\n",
    "\n",
    "    # cdef double[:,::1] C = np.ones((N, K), dtype=np.float32) * DBL_MAX\n",
    "    C = np.ones((N+1, K+1, max_hop), dtype=np.double, order=\"C\") * 100000\n",
    "    B = np.ones((N+1, K+1, max_hop), dtype=np.int32)\n",
    "    \n",
    "    for i in range(min_hop, min(max_hop,N)):\n",
    "        C[i,0,i] = 0.\n",
    "            \n",
    "    dp = np.zeros((N+1,N+1), dtype=np.double, order=\"C\")\n",
    "    cumsum = ssm.cumsum(axis=0).cumsum(axis=1)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            dp[i+1,j+1] = cumsum[i,j] \n",
    "    \n",
    "    for i in range(2*min_hop, N+1):  # exclusive index of last boundary 1 means only hits 0, etc. Start at 2nd cut.\n",
    "        for l in range(min_hop, max_hop):\n",
    "            for k in range(1,K):\n",
    "                best_val = 100000\n",
    "                best_ind = -1\n",
    "                for j in range(min_hop, max_hop):\n",
    "                    a,b,c = i-l-j, i-l, i\n",
    "                    if a < 0 or b < min_hop:\n",
    "                        continue\n",
    "                    on_1 = dp[c,c] - dp[c,b] - dp[b,c] + dp[b,b] \n",
    "                    off_1 = dp[c,b] - dp[b,b] - dp[c,a] + dp[b,a] \n",
    "                    off_2 = dp[b,c] - dp[a,c] - dp[b,b] + dp[a,b] \n",
    "                    on_2 = dp[b,b] - dp[b,a] - dp[a,b] + dp[a,a]\n",
    "                    \n",
    "                    on_1 /= (c-b)#*(c-b)\n",
    "                    off_1 /= np.sqrt((c-b)*(b-a))\n",
    "                    off_2 /= np.sqrt((b-a)*(c-b))\n",
    "                    on_2 /= (b-a)#*(b-a)\n",
    "#                     print(on_1, on_2, off_1, off_2, a,b,c)\n",
    "#                     print((2/(1/on_1+1/on_2)))\n",
    "                    \n",
    "                    cur_val = (C[i-l, k-1, j] + (off_1+off_2)-(on_1+on_2))                    \n",
    "                    if cur_val < best_val:  # todo add length bias here\n",
    "                        best_val = cur_val\n",
    "                        best_ind = l\n",
    "                    \n",
    "\n",
    "\n",
    "                B[i,k,l] = best_ind\n",
    "                C[i,k,l] = best_val\n",
    "    \n",
    "    # backtrack\n",
    "    boundary = []\n",
    "    prev_b = N\n",
    "    prev_j = np.argmin(C[N, K-1])\n",
    "    print(np.min(C[N, K-1]))\n",
    "    loop = list(range(K))[::-1][:-1]\n",
    "    boundary.append(prev_b)\n",
    "#     print(prev_j, np.min(C[N, K-1]))\n",
    "    for k in loop:\n",
    "        print(prev_b, k, prev_j, C[prev_b, k, prev_j])\n",
    "        temp = B[prev_b,k,prev_j]\n",
    "        prev_b = prev_b - prev_j\n",
    "        prev_j = temp\n",
    "        boundary.append(prev_b)\n",
    "    boundary = boundary[::-1] # reverse\n",
    "    # boundary = boundary[1:-1] # chop start and end\n",
    "    # boundary = [item - 0.5 for item in boundary] # adjust\n",
    "    # boundary[0], boundary[-1] = boundary[0] + 0.5, boundary[-1] + 0.5\n",
    "    \n",
    "    return boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_hubert_losses(audio, padding_mask, labels_tensor, mask):\n",
    "    results = hubert_large_model(\n",
    "        audio, \n",
    "        padding_mask=padding_mask, \n",
    "        target_list = [labels_tensor], \n",
    "        features_only=False,\n",
    "        mask_override=mask.cuda(),\n",
    "    )\n",
    "    losses = F.cross_entropy(hubert_large_model.get_logits(results, True)[0], labels_tensor[mask]+5, reduction='none')\n",
    "    return losses\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_full_losses(audio, padding_mask, labels_tensor, mask):\n",
    "    masked_losses = get_hubert_losses(audio, padding_mask, labels_tensor, mask).unsqueeze(0).cpu()\n",
    "    losses = torch.zeros(mask.shape)\n",
    "    losses[mask] = masked_losses\n",
    "    return losses, masked_losses\n",
    "\n",
    "def hubert_plot_1(key, tensor):\n",
    "    plot_featmap(None, word_ali=get_word_ali(item[key], fps=50)\n",
    "         , start_in_sec=0, fps=50, image_already=tensor.cpu().repeat(15,1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877de688",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "window_size = 50  # odd number\n",
    "ssl = window_size//2\n",
    "ssr = window_size//2 + 1\n",
    "spf = 1/50\n",
    "default_window = 5\n",
    "\n",
    "\n",
    "\n",
    "def slow_get_hubert_losses(\n",
    "    audio_file,\n",
    "    root='/data/scratch/pyp/datasets/coco_pyp/SpokenCOCO/',\n",
    "    use_vad=True,\n",
    "    item=None,\n",
    "):\n",
    "    audio_file_path = root + audio_file\n",
    "    \n",
    "    if use_vad:\n",
    "        audio, sr = torchaudio.load(audio_file_path)\n",
    "        start_sec, end_sec, nonzero = get_vad_full_boundaries(audio.view(-1).numpy(), sr)\n",
    "        audio = audio.cuda()\n",
    "#         times = item['text_alignment'].split('__')\n",
    "        min_t, max_t = [int(a*sr) for a in [start_sec, end_sec]]\n",
    "        audio = audio[:,min_t:max_t]\n",
    "        \n",
    "        LAYER=9\n",
    "        base_results = get_hubert_results(audio)\n",
    "        base_layer_result = base_results['hidden_states'][LAYER][0].transpose(0,1)[0]\n",
    "        labels_tensor = hubert_akm(base_layer_result).cuda()\n",
    "        T = labels_tensor.size(-1)\n",
    "        padding_mask = audio.new_zeros(audio.shape[:2], dtype=bool)\n",
    "    else:\n",
    "        # Get labels\n",
    "        LAYER=9  # todo faster\n",
    "        base_results = get_hubert_results(audio_file_path)\n",
    "        base_layer_result = base_results['hidden_states'][LAYER][0].transpose(0,1)[0]\n",
    "        \n",
    "        labels_tensor = hubert_akm(base_layer_result).cuda()\n",
    "        T = labels_tensor.size(-1)\n",
    "\n",
    "        # Get hubert loss\n",
    "        audio, sr = torchaudio.load(audio_file_path)\n",
    "        audio = audio.cuda()\n",
    "        padding_mask = audio.new_zeros(audio.shape[:2], dtype=bool)\n",
    "\n",
    "    mask = torch.zeros(1, T).bool()\n",
    "    full_losses = torch.zeros(T,T)  # from, to\n",
    "    default_loss = torch.zeros(T)\n",
    "    default_count = torch.zeros(T)\n",
    "    \n",
    "    for mid in tqdm(range(-ssl,T+ssr)):\n",
    "        mask = torch.zeros((T,)).bool()\n",
    "        mask[max(0,mid-ssl):min(T,mid+ssr)] = 1\n",
    "        losses, masked_losses = get_full_losses(audio, padding_mask, labels_tensor, mask.unsqueeze(0))\n",
    "\n",
    "        if mid-ssl-1 >= 0:\n",
    "            losses1 = losses.clone().view(-1)\n",
    "            losses1[min(T,mid):] = 0\n",
    "            full_losses[mid-ssl-1] += losses1\n",
    "\n",
    "        if mid+ssr < T:\n",
    "            losses2 = losses.clone().view(-1)\n",
    "            losses2[:max(0,mid)] = 0\n",
    "            full_losses[mid+ssr] += losses2\n",
    "\n",
    "        cur_default_loss = losses.clone()[0]\n",
    "        cur_default_loss[:max(0,mid-default_window)] = 0\n",
    "        cur_default_loss[min(T, max(0,mid+default_window+1)):] = 0\n",
    "        default_loss += cur_default_loss\n",
    "        default_count += (cur_default_loss > 0).int()\n",
    "        \n",
    "    default_losses = (default_loss / default_count).unsqueeze(0)\n",
    "    \n",
    "    return full_losses, default_losses, base_layer_result, audio, start_sec, end_sec, nonzero, base_layer_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a6da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hubert_large_model.half()\n",
    "@torch.inference_mode()\n",
    "def fast_get_hubert_losses(\n",
    "    audio_file,\n",
    "    root='/data/scratch/pyp/datasets/coco_pyp/SpokenCOCO/',\n",
    "    item=None,\n",
    "    window_size = 51,  # odd number\n",
    "    batch_size = 32,\n",
    "    gap_size = None,\n",
    "):\n",
    "    ssl = window_size//2\n",
    "    ssr = window_size//2+1\n",
    "    spf = 1/50\n",
    "    default_window = 5\n",
    "    LAYER=9\n",
    "    \n",
    "    audio_file_path = root + audio_file\n",
    "    \n",
    "    audio, sr = torchaudio.load(audio_file_path)\n",
    "    start_sec, end_sec, nonzero = get_vad_full_boundaries(audio.view(-1).numpy(), sr)\n",
    "    audio = audio.cuda()\n",
    "    min_t, max_t = [int(a*sr) for a in [start_sec, end_sec]]\n",
    "    audio = audio[:,min_t:max_t]\n",
    "\n",
    "    base_results = get_hubert_results(audio)\n",
    "    base_layer_result = base_results['hidden_states'][LAYER][0].transpose(0,1)[0]\n",
    "    labels_tensor = hubert_akm(base_layer_result).cuda()\n",
    "    T = labels_tensor.size(-1)\n",
    "    padding_mask = audio.new_zeros(audio.shape[:2], dtype=bool)\n",
    "\n",
    "    full_losses = torch.zeros(T,T)  # from, to\n",
    "    default_loss = torch.zeros(T)\n",
    "    default_count = torch.zeros(T)\n",
    "    N = T+ssr+ssl-1\n",
    "    all_losses = torch.zeros(N,T)  # from, to\n",
    "        \n",
    "    if gap_size is not None and gap_size >= T:\n",
    "        gap_size = None\n",
    "    \n",
    "    if gap_size is None:\n",
    "        mask = torch.zeros(N,T,dtype=bool)\n",
    "        for idx, mid in enumerate(range(-ssl,T+ssr-1)):\n",
    "            mask[idx, max(0,mid-ssl):min(T,mid+ssr)] = True\n",
    "\n",
    "        audio = audio.half()\n",
    "\n",
    "        for idx in range(0, N, batch_size):\n",
    "            bs = min(N, idx+batch_size) - idx\n",
    "            losses, _ = get_full_losses(audio.expand(bs,-1), padding_mask.expand(bs,-1), labels_tensor.expand(bs,-1), mask[idx:idx+bs])\n",
    "            all_losses[idx:idx+bs] = losses\n",
    "    else:\n",
    "        mask = torch.zeros(gap_size,T,dtype=bool)\n",
    "        \n",
    "        for idx, mid in enumerate(range(-ssl,-ssl+gap_size)):\n",
    "            for mid2 in range(mid, T+ssr-1, gap_size):\n",
    "                mask[idx, max(0,mid2-ssl):min(T,mid2+ssr)] = True\n",
    "        audio = audio.half()\n",
    "\n",
    "        for idx in range(0, gap_size, batch_size):\n",
    "            bs = min(gap_size, idx+batch_size) - idx\n",
    "            \n",
    "            cur_mask = mask[idx:idx+bs].cuda()\n",
    "            cur_labels = labels_tensor.expand(bs,-1)\n",
    "            features_only = hubert_large_model(\n",
    "                audio.expand(bs,-1), \n",
    "                padding_mask=padding_mask.expand(bs,-1), \n",
    "                target_list = [cur_labels], \n",
    "                features_only=True,\n",
    "                mask_override=cur_mask,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            features = hubert_large_model.final_proj(features_only['x'])\n",
    "            x = (features/((features**2).sum(dim=-1, keepdim=True).sqrt()+1e-5) @ \n",
    "                 (hubert_large_model.label_embs_concat/((hubert_large_model.label_embs_concat**2)\n",
    "                                                        .sum(dim=-1, keepdim=True).sqrt() + 1e-5)).unsqueeze(0).transpose(-1,-2)\n",
    "                )\n",
    "            x = x / hubert_large_model.logit_temp\n",
    "            losses = x.new_zeros(cur_mask.shape)\n",
    "            losses[cur_mask] = F.cross_entropy(x[cur_mask], cur_labels[cur_mask]+4, reduction='none')\n",
    "            \n",
    "            for idx2 in range(idx, N, gap_size):\n",
    "                all_losses[idx2:min(N,idx2+bs)] = losses[:min(bs, N-idx2)]\n",
    "        \n",
    "    \n",
    "    for idx, mid in enumerate(range(-ssl,T+ssr-1)):\n",
    "        losses = all_losses[idx]\n",
    "        if mid-ssl-1 >= 0:\n",
    "            losses1 = losses.clone().view(-1)\n",
    "            losses1[min(T,mid):] = 0\n",
    "            full_losses[mid-ssl-1] += losses1\n",
    "\n",
    "        if mid+ssr < T:\n",
    "            losses2 = losses.clone().view(-1)\n",
    "            losses2[:max(0,mid)] = 0\n",
    "            full_losses[mid+ssr] += losses2\n",
    "\n",
    "        cur_default_loss = losses.clone()\n",
    "        cur_default_loss[:max(0,mid-default_window)] = 0\n",
    "        cur_default_loss[min(T, max(0,mid+default_window+1)):] = 0\n",
    "        default_loss += cur_default_loss\n",
    "        default_count += (cur_default_loss > 0).int()\n",
    "        \n",
    "    full_losses = full_losses.tril(ssr+2).triu(-(ssr+2))\n",
    "    default_losses = (default_loss / default_count).unsqueeze(0)\n",
    "    \n",
    "    return full_losses, default_losses, base_layer_result, audio, start_sec, end_sec, nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4af90e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting a 2d loss matrix efficiently from off-the-shelf HuBERT models\n",
    "\n",
    "t1 = time.time()\n",
    "full_losses, default_losses, base_layer_result, audio, start_sec, end_sec, nonzero = fast_get_hubert_losses(\n",
    "    item['caption']['wav'], \n",
    "    root=\"\", \n",
    "    batch_size=32,\n",
    "    gap_size=150,\n",
    ")\n",
    "print(time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "cimport cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "np.import_array()\n",
    "from libc.math cimport sqrt\n",
    "from cython.operator import postincrement as inc\n",
    "from cpython.mem cimport PyMem_Malloc, PyMem_Free\n",
    "\n",
    "cdef extern from \"float.h\":\n",
    "    double DBL_MAX\n",
    "\n",
    "    \n",
    "    \n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.cdivision(True)\n",
    "def min_cut_c(np.ndarray ssm, Py_ssize_t Ks, double bias=0., min_hop=1, allow_skip=0, weird=0.075, can_skip=0):\n",
    "    cdef Py_ssize_t K = np.max(Ks)\n",
    "    cdef Py_ssize_t N = ssm.shape[0]\n",
    "\n",
    "    # cdef double[:,::1] C = np.ones((N, K), dtype=np.float32) * DBL_MAX\n",
    "    cdef double[:,::1] C = np.ones((N+1, K), dtype=np.float32, order=\"C\") * DBL_MAX\n",
    "    cdef int[:,::1] B = np.ones((N+1, K), dtype=np.int32)\n",
    "\n",
    "    C[0,0] = 0.\n",
    "#     C[0,0] = -np.inf\n",
    "    \n",
    "    cdef list temp, obj\n",
    "    cdef Py_ssize_t i, j, k, ind, a\n",
    "    \n",
    "    # kinda slow copy\n",
    "    cdef double[:,:] dp = np.zeros((N+1,N+1), dtype=np.double, order=\"C\")\n",
    "    cumsum = ssm.cumsum(axis=0).cumsum(axis=1)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            dp[1+i,1+j] = cumsum[i,j] \n",
    "\n",
    "    for i in range(min_hop, N+1-allow_skip):\n",
    "        temp = [(\n",
    "            (dp[i,i]-dp[j,i]-dp[i,j]+dp[j,j]),  # interior\n",
    "            (dp[i,j]-dp[j,j])*1.0,  # left  0.75\n",
    "            (dp[j,i]-dp[j,j])*1.0,  # top\n",
    "            (dp[N,i]-dp[i,i]-dp[N,j]+dp[i,j])*1.0,  # bottom\n",
    "            (dp[i,N]-dp[i,i]-dp[j,N]+dp[j,i])*1.0,  # right\n",
    "            ) for j in range(i-min_hop+1)\n",
    "        ]\n",
    "        for k in range(1,K):\n",
    "            obj = [C[max(0,j-can_skip), k-1] + (((item[1]+item[2]+item[3]+item[4])/2.)/((item[1]+item[2]+item[3]+item[4])/2.+item[0]/2.+1e-5)) for j, item in enumerate(temp)]\n",
    "            ind = np.argmin(obj)\n",
    "            B[i+allow_skip,k] = ind\n",
    "            C[i+allow_skip,k] = obj[ind]\n",
    "\n",
    "    \n",
    "    # backtrack\n",
    "    cdef list boundary = []\n",
    "#     cdef Py_ssize_t prev_b = N - 1\n",
    "    cdef Py_ssize_t prev_b = N\n",
    "#     while K > 5 and C[N,K-1]/K >= weird:\n",
    "#         K -= 1\n",
    "    cdef list loop = list(range(K))[::-1][:-1]\n",
    "    boundary.append(prev_b)\n",
    "    for k in loop:\n",
    "        prev_b = B[prev_b,k]\n",
    "        boundary.append(prev_b)\n",
    "    boundary = boundary[::-1] # reverse\n",
    "    # boundary = boundary[1:-1] # chop start and end\n",
    "    # boundary = [item - 0.5 for item in boundary] # adjust\n",
    "    # boundary[0], boundary[-1] = boundary[0] + 0.5, boundary[-1] + 0.5\n",
    "    return boundary, C[N,K-1]*1.0/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ca537",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "cimport cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "np.import_array()\n",
    "from libc.math cimport sqrt\n",
    "from cython.operator import postincrement as inc\n",
    "from cpython.mem cimport PyMem_Malloc, PyMem_Free\n",
    "\n",
    "cdef extern from \"float.h\":\n",
    "    double DBL_MAX\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.cdivision(True)\n",
    "def min_cut_efficient_c(np.ndarray dp, Py_ssize_t K, min_hop=3, max_hop=40):\n",
    "    cdef Py_ssize_t N = dp.shape[0] - 1\n",
    "\n",
    "    cdef double[:,::1] C = np.ones((N+1, K), dtype=np.float32, order=\"C\") * DBL_MAX\n",
    "    cdef int[:,::1] B = np.ones((N+1, K), dtype=np.int32)\n",
    "    C[0,0] = 0.\n",
    "    \n",
    "    cdef list temp, obj\n",
    "    cdef Py_ssize_t i, j, k, ind, a, best_ind\n",
    "    cdef double best_val, cur_val\n",
    "    \n",
    "    for i in range(min_hop, N+1):\n",
    "        temp = []\n",
    "        for j in range(max(0, i-max_hop+1), i-min_hop+1):\n",
    "            item = (\n",
    "                (dp[i,i]-dp[j,i]-dp[i,j]+dp[j,j]),  # interior\n",
    "                (dp[i,j]-dp[j,j]),  # left\n",
    "                (dp[j,i]-dp[j,j]),  # top\n",
    "                (dp[N,i]-dp[i,i]-dp[N,j]+dp[i,j]),  # bottom\n",
    "                (dp[i,N]-dp[i,i]-dp[j,N]+dp[j,i]),  # right\n",
    "            )\n",
    "            temp.append((j, (item[1]+item[2]+item[3]+item[4])/(item[1]+item[2]+item[3]+item[4]+item[0]+1e-5)))\n",
    "        for k in range(1,K):\n",
    "            obj = [C[j, k-1] + item for (j, item) in temp]\n",
    "            ind = np.argmin(obj)\n",
    "            B[i,k] = temp[ind][0]\n",
    "            C[i,k] = obj[ind]\n",
    "\n",
    "    \n",
    "    cdef list boundary = []\n",
    "    cdef Py_ssize_t prev_b = N\n",
    "    cdef list loop = list(range(K))[::-1][:-1]\n",
    "    boundary.append(prev_b)\n",
    "    for k in loop:\n",
    "        prev_b = B[prev_b,k]\n",
    "        boundary.append(prev_b)\n",
    "    boundary = boundary[::-1]\n",
    "\n",
    "    return boundary, C[N,K-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de07f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_cut_efficient(dp, Ks, min_hop=3, max_hop=50):\n",
    "    K = max(Ks)\n",
    "    N = dp.shape[0] - 1\n",
    "\n",
    "    C = np.ones((N+1, K), dtype=np.float32, order=\"C\") * 100000\n",
    "    B = np.ones((N+1, K), dtype=np.int32)\n",
    "    C[0,0] = 0.\n",
    "    \n",
    "    for i in range(min_hop, N+1):\n",
    "        temp = []\n",
    "        for j in range(max(0, i-max_hop+1), i-min_hop+1):\n",
    "            item = (\n",
    "                (dp[i,i]-dp[j,i]-dp[i,j]+dp[j,j]),  # interior\n",
    "                (dp[i,j]-dp[j,j]),  # left\n",
    "                (dp[j,i]-dp[j,j]),  # top\n",
    "                (dp[N,i]-dp[i,i]-dp[N,j]+dp[i,j]),  # bottom\n",
    "                (dp[i,N]-dp[i,i]-dp[j,N]+dp[j,i]),  # right\n",
    "            )\n",
    "            temp.append((j, (item[1]+item[2]+item[3]+item[4])/(item[1]+item[2]+item[3]+item[4]+item[0]+1e-5)))\n",
    "        for k in range(1,K):\n",
    "            obj = [C[j, k-1] + item for (j, item) in temp]\n",
    "            ind = np.argmin(obj)\n",
    "            B[i,k] = temp[ind][0]\n",
    "            C[i,k] = obj[ind]\n",
    "\n",
    "    \n",
    "    boundaries = []\n",
    "    for K in Ks:\n",
    "        boundary = []\n",
    "        prev_b = N\n",
    "        loop = list(range(K))[::-1][:-1]\n",
    "        boundary.append(prev_b)\n",
    "        for k in loop:\n",
    "            prev_b = B[prev_b,k]\n",
    "            boundary.append(prev_b)\n",
    "        boundary = boundary[::-1]\n",
    "        \n",
    "        boundaries.append(boundary)\n",
    "\n",
    "    return boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient loss pred extraction code from precalculated losses\n",
    "\n",
    "def clean_log_mi_exp(nlog_mi, min_clip=-10):\n",
    "    log_mi = -nlog_mi\n",
    "    for row in range(log_mi.size(0)):\n",
    "        log_mi[row, min(log_mi.size(0), row+ssr):] = min_clip\n",
    "        log_mi[row, :max(0, row-ssl)] = min_clip\n",
    "        log_mi[row, row] = min_clip    \n",
    "    log_mi = log_mi.clamp(min=min_clip).exp()\n",
    "    \n",
    "    return log_mi\n",
    "\n",
    "\n",
    "def set_diagonal(x, v, dist=None):\n",
    "    x = x.clone()\n",
    "    for row in range(x.size(0)):\n",
    "        x[row, min(x.size(0), row+(ssr if dist is None else dist+1)):] = v\n",
    "        x[row, :max(0, row-(ssl if dist is None else dist))] = v\n",
    "    return x\n",
    "\n",
    "def process_loss_multiple(\n",
    "    full_losses, \n",
    "    default_losses,\n",
    "    b1 = 0,\n",
    "    b2 = 0,\n",
    "    m1 = 1,\n",
    "    m11 = 1,\n",
    "    m2 = 0,\n",
    "    m21 = 0,\n",
    "    m3 = 1,\n",
    "    m4 = 1,\n",
    "    y1 = 0.5,\n",
    "    y2 = 0.5,\n",
    "    y3 = 0.5,\n",
    "    y4 = 0.0,\n",
    "    y5 = 0.0,\n",
    "    sec_per_syllables = [0.15],\n",
    "    sec_shift = 0,\n",
    "    max_clip = =100,\n",
    "    mincut_bias = 0,\n",
    "    mincut_minhop = 1,\n",
    "    silent_sec = None,\n",
    "):\n",
    "    full_losses = set_diagonal(full_losses, -max_clip/m2)\n",
    "    gain = full_losses\n",
    "\n",
    "    T = gain.size(0)\n",
    "    ks = [int(T / 50 / sec_per_syllable) + 1 for sec_per_syllable in sec_per_syllables]\n",
    "    spf = 1/50\n",
    "\n",
    "    cur_feats = clean_log_mi_exp(gain, min_clip=max_clip)\n",
    "\n",
    "    dp = np.zeros((cur_feats.size(0) + 1, cur_feats.size(1)+1))\n",
    "    dp[1:,1:] = cur_feats.cumsum(dim=0).cumsum(dim=1)\n",
    "\n",
    "    min_cut_boundaries = min_cut_efficient(dp, ks, mincut_minhop)\n",
    "    \n",
    "    word_boundarieses = []\n",
    "    for min_cut_boundary in min_cut_boundaries:    \n",
    "    \n",
    "        word_boundaries = [[l*spf+sec_shift,r*spf+sec_shift] for l, r in zip(min_cut_boundary[:-1], min_cut_boundary[1:])]\n",
    "\n",
    "        if silent_sec is not None:\n",
    "            word_boundaries = [wb for wb in word_boundaries if abs(wb[0]-silent_sec[np.searchsorted(silent_sec, wb[0])]) > 0.001]\n",
    "\n",
    "        word_boundarieses.append(word_boundaries)\n",
    "            \n",
    "    return word_boundarieses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate LossPred Over a Dataset\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "CUR_DATA= ls_aligned_dev[:500]\n",
    "\n",
    "abc = []\n",
    "for item in tqdm(CUR_DATA):\n",
    "    a,b,_,audio,c,d,e = fast_get_hubert_losses(item['caption']['wav'], root=\"\", gap_size=150)\n",
    "    word_boundaries = process_loss_multiple(\n",
    "        a,\n",
    "        b,\n",
    "        sec_shift=c,\n",
    "        m1=0.5,\n",
    "        m2=0.5,\n",
    "        y1=0.0,\n",
    "        y2=0.0,\n",
    "        sec_per_syllables=[0.16], # safe\n",
    "        mincut_minhop=1,\n",
    "        max_clip=-100,\n",
    "        silent_sec = e/100.\n",
    "    )[0]\n",
    "    \n",
    "    data_dict[item['caption']['wav']] = {'word_boundaries': word_boundaries}\n",
    "    _ = word_boundaries[-1][-1]/len(word_boundaries)\n",
    "    abc.append(_)\n",
    "    \n",
    "get_slow_coco_outs(data_dict, CUR_DATA, level='syllable', tolerance=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
